<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-vla/generative-robotics" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Generative Robotics: LLMs and Control | Sentient Machines</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://CodeVoyager007.github.io/BOOK-WITH-RAGCHATBOT/img/logo.png"><meta data-rh="true" name="twitter:image" content="https://CodeVoyager007.github.io/BOOK-WITH-RAGCHATBOT/img/logo.png"><meta data-rh="true" property="og:url" content="https://CodeVoyager007.github.io/BOOK-WITH-RAGCHATBOT/docs/vla/generative-robotics"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Generative Robotics: LLMs and Control | Sentient Machines"><meta data-rh="true" name="description" content="The field of robotics is undergoing a profound transformation with the advent of generative AI, particularly Large Language Models (LLMs). Traditionally, robots are programmed with explicit instructions or learned policies for specific tasks. Generative robotics, however, aims to leverage LLMs to enable robots to understand high-level human commands, reason about complex situations, and generate novel behaviors on the fly, bridging the gap between human intent and robotic action."><meta data-rh="true" property="og:description" content="The field of robotics is undergoing a profound transformation with the advent of generative AI, particularly Large Language Models (LLMs). Traditionally, robots are programmed with explicit instructions or learned policies for specific tasks. Generative robotics, however, aims to leverage LLMs to enable robots to understand high-level human commands, reason about complex situations, and generate novel behaviors on the fly, bridging the gap between human intent and robotic action."><link data-rh="true" rel="icon" href="/BOOK-WITH-RAGCHATBOT/img/logo.png"><link data-rh="true" rel="canonical" href="https://CodeVoyager007.github.io/BOOK-WITH-RAGCHATBOT/docs/vla/generative-robotics"><link data-rh="true" rel="alternate" href="https://CodeVoyager007.github.io/BOOK-WITH-RAGCHATBOT/docs/vla/generative-robotics" hreflang="en"><link data-rh="true" rel="alternate" href="https://CodeVoyager007.github.io/BOOK-WITH-RAGCHATBOT/docs/vla/generative-robotics" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Vision-Language-Action (VLA)","item":"https://CodeVoyager007.github.io/BOOK-WITH-RAGCHATBOT/docs/category/vla"},{"@type":"ListItem","position":2,"name":"Generative Robotics","item":"https://CodeVoyager007.github.io/BOOK-WITH-RAGCHATBOT/docs/vla/generative-robotics"}]}</script><link rel="stylesheet" href="/BOOK-WITH-RAGCHATBOT/assets/css/styles.6fa444b3.css">
<script src="/BOOK-WITH-RAGCHATBOT/assets/js/runtime~main.9bf2e1d6.js" defer="defer"></script>
<script src="/BOOK-WITH-RAGCHATBOT/assets/js/main.5c8bba61.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/BOOK-WITH-RAGCHATBOT/"><div class="navbar__logo"><img src="/BOOK-WITH-RAGCHATBOT/img/logo.png" alt="Sentient Machines Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/BOOK-WITH-RAGCHATBOT/img/logo.png" alt="Sentient Machines Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Sentient Machines</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/BOOK-WITH-RAGCHATBOT/docs/category/foundations">Handbook</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/CodeVoyager007/BOOK-WITH-RAGCHATBOT" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/BOOK-WITH-RAGCHATBOT/docs/category/foundations"><span title="The Awakening (Foundations)" class="categoryLinkLabel_W154">The Awakening (Foundations)</span></a><button aria-label="Expand sidebar category &#x27;The Awakening (Foundations)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/BOOK-WITH-RAGCHATBOT/docs/category/ros-2"><span title="The Nervous System (ROS 2)" class="categoryLinkLabel_W154">The Nervous System (ROS 2)</span></a><button aria-label="Expand sidebar category &#x27;The Nervous System (ROS 2)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/BOOK-WITH-RAGCHATBOT/docs/category/simulation"><span title="Digital Twins (Simulation)" class="categoryLinkLabel_W154">Digital Twins (Simulation)</span></a><button aria-label="Expand sidebar category &#x27;Digital Twins (Simulation)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/BOOK-WITH-RAGCHATBOT/docs/category/isaac"><span title="The AI Brain (NVIDIA Isaac)" class="categoryLinkLabel_W154">The AI Brain (NVIDIA Isaac)</span></a><button aria-label="Expand sidebar category &#x27;The AI Brain (NVIDIA Isaac)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/BOOK-WITH-RAGCHATBOT/docs/category/vla"><span title="Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Vision-Language-Action (VLA)</span></a><button aria-label="Collapse sidebar category &#x27;Vision-Language-Action (VLA)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/BOOK-WITH-RAGCHATBOT/docs/vla/generative-robotics"><span title="Generative Robotics" class="linkLabel_WmDU">Generative Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/BOOK-WITH-RAGCHATBOT/docs/vla/voice-command"><span title="Voice Command" class="linkLabel_WmDU">Voice Command</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/BOOK-WITH-RAGCHATBOT/docs/vla/cognitive-planning"><span title="Cognitive Planning" class="linkLabel_WmDU">Cognitive Planning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/BOOK-WITH-RAGCHATBOT/docs/vla/vla-models"><span title="VLA Models" class="linkLabel_WmDU">VLA Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/BOOK-WITH-RAGCHATBOT/docs/vla/zero-shot"><span title="Zero-Shot Control" class="linkLabel_WmDU">Zero-Shot Control</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/BOOK-WITH-RAGCHATBOT/docs/category/capstone"><span title="The Capstone &amp; Deployment" class="categoryLinkLabel_W154">The Capstone &amp; Deployment</span></a><button aria-label="Expand sidebar category &#x27;The Capstone &amp; Deployment&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/BOOK-WITH-RAGCHATBOT/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/BOOK-WITH-RAGCHATBOT/docs/category/vla"><span>Vision-Language-Action (VLA)</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Generative Robotics</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Generative Robotics: The Convergence of LLMs and Control</h1></header>
<p>The field of robotics is undergoing a profound transformation with the advent of <strong>generative AI</strong>, particularly <strong>Large Language Models (LLMs)</strong>. Traditionally, robots are programmed with explicit instructions or learned policies for specific tasks. Generative robotics, however, aims to leverage LLMs to enable robots to understand high-level human commands, reason about complex situations, and generate novel behaviors on the fly, bridging the gap between human intent and robotic action.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-gap-human-language-to-robot-action">The Gap: Human Language to Robot Action<a href="#the-gap-human-language-to-robot-action" class="hash-link" aria-label="Direct link to The Gap: Human Language to Robot Action" title="Direct link to The Gap: Human Language to Robot Action" translate="no">â€‹</a></h2>
<p>Humans communicate intent through natural language. Robots, however, operate on low-level commands: joint angles, motor torques, or velocity commands. Translating a high-level instruction like &quot;Please make me a coffee&quot; into a sequence of robot actions (navigate to kitchen, open cabinet, grasp mug, fill with water, place in machine, press button) is a monumental challenge.</p>
<p>Traditional approaches rely on:</p>
<ul>
<li class=""><strong>Hardcoded State Machines</strong>: Pre-defined sequences for common tasks.</li>
<li class=""><strong>Task and Motion Planning (TAMP)</strong>: Sophisticated planners that search for valid sequences of actions given a model of the world and robot capabilities.</li>
<li class=""><strong>Reinforcement Learning</strong>: Learning policies for specific skills, often requiring extensive training.</li>
</ul>
<p>These methods are brittle, lack generalization, and struggle with novel situations or ambiguous instructions.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-llm-revolution-in-robotics">The LLM Revolution in Robotics<a href="#the-llm-revolution-in-robotics" class="hash-link" aria-label="Direct link to The LLM Revolution in Robotics" title="Direct link to The LLM Revolution in Robotics" translate="no">â€‹</a></h2>
<p>LLMs, with their vast knowledge base of human language and common sense, offer a promising solution to this &quot;language-to-action&quot; gap. They can:</p>
<ol>
<li class=""><strong>Interpret High-Level Goals</strong>: Break down complex human commands into a sequence of sub-goals or logical steps.</li>
<li class=""><strong>Reason about Affordances</strong>: Understand what objects can be used for (e.g., a mug can hold liquid, a door can be opened).</li>
<li class=""><strong>Generate Code or Plans</strong>: Translate abstract steps into executable robot code, API calls, or symbolic plans that traditional robotic planners can understand.</li>
<li class=""><strong>Handle Ambiguity</strong>: Ask clarifying questions or make reasonable assumptions based on context.</li>
</ol>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>LLMs as High-Level Planners</div><div class="admonitionContent_BuS1"><p>Instead of programming each step, the LLM acts as a high-level planner, taking an abstract goal and recursively breaking it down into sub-goals until they are simple enough for existing robot skills (e.g., &quot;grasp object&quot;, &quot;navigate to location&quot;) to execute.</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="architectures-for-generative-robotics">Architectures for Generative Robotics<a href="#architectures-for-generative-robotics" class="hash-link" aria-label="Direct link to Architectures for Generative Robotics" title="Direct link to Architectures for Generative Robotics" translate="no">â€‹</a></h2>
<p>Several architectural patterns are emerging to integrate LLMs into robotic control:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-code-generation">1. Code Generation<a href="#1-code-generation" class="hash-link" aria-label="Direct link to 1. Code Generation" title="Direct link to 1. Code Generation" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Concept</strong>: The LLM directly generates executable code (e.g., Python scripts that use ROS 2 APIs) based on human instructions.</li>
<li class=""><strong>Pros</strong>: Highly flexible, can adapt to new skills if the LLM has access to documentation.</li>
<li class=""><strong>Cons</strong>: Requires a robust code execution and safety monitoring system. The generated code must be correct and safe.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-api-call-generation">2. API Call Generation<a href="#2-api-call-generation" class="hash-link" aria-label="Direct link to 2. API Call Generation" title="Direct link to 2. API Call Generation" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Concept</strong>: The LLM is given access to a library of robot &quot;skills&quot; (e.g., <code>navigate(location)</code>, <code>grasp(object)</code>, <code>open(door)</code>), each exposed as an API function. The LLM generates sequences of these API calls.</li>
<li class=""><strong>Pros</strong>: More constrained and safer than full code generation. Easier to verify the LLM&#x27;s output.</li>
<li class=""><strong>Cons</strong>: Limited by the predefined API skills. New skills require updating the API.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-symbolic-planning">3. Symbolic Planning<a href="#3-symbolic-planning" class="hash-link" aria-label="Direct link to 3. Symbolic Planning" title="Direct link to 3. Symbolic Planning" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Concept</strong>: The LLM generates a symbolic plan (a sequence of high-level actions) that is then fed to a traditional Task and Motion Planner (TAMP) system. The TAMP system grounds these symbolic actions into executable robot movements.</li>
<li class=""><strong>Pros</strong>: Combines LLM&#x27;s reasoning with TAMP&#x27;s robust grounding and collision avoidance.</li>
<li class=""><strong>Cons</strong>: Requires a formal domain model (PDDL-like) that the LLM must understand.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="vision-language-action-vla-models">Vision-Language-Action (VLA) Models<a href="#vision-language-action-vla-models" class="hash-link" aria-label="Direct link to Vision-Language-Action (VLA) Models" title="Direct link to Vision-Language-Action (VLA) Models" translate="no">â€‹</a></h2>
<p>The next evolution involves tightly integrating vision, language, and action into a single model. Instead of separate modules for perception, planning, and control, VLA models aim to learn an end-to-end mapping from visual observations and language instructions directly to robot actions. These models will be explored in detail in later chapters.</p>
<p>Generative robotics is still in its early stages, but it promises to unlock a new era of robot autonomy, where robots can understand and assist humans in more natural and intuitive ways, moving beyond programmed behaviors to truly intelligent and adaptable action.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/CodeVoyager007/BOOK-WITH-RAGCHATBOT/tree/main/book/docs/05-vla/01-generative-robotics.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/BOOK-WITH-RAGCHATBOT/docs/category/vla"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Vision-Language-Action (VLA)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/BOOK-WITH-RAGCHATBOT/docs/vla/voice-command"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Voice Command</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-gap-human-language-to-robot-action" class="table-of-contents__link toc-highlight">The Gap: Human Language to Robot Action</a></li><li><a href="#the-llm-revolution-in-robotics" class="table-of-contents__link toc-highlight">The LLM Revolution in Robotics</a></li><li><a href="#architectures-for-generative-robotics" class="table-of-contents__link toc-highlight">Architectures for Generative Robotics</a><ul><li><a href="#1-code-generation" class="table-of-contents__link toc-highlight">1. Code Generation</a></li><li><a href="#2-api-call-generation" class="table-of-contents__link toc-highlight">2. API Call Generation</a></li><li><a href="#3-symbolic-planning" class="table-of-contents__link toc-highlight">3. Symbolic Planning</a></li></ul></li><li><a href="#vision-language-action-vla-models" class="table-of-contents__link toc-highlight">Vision-Language-Action (VLA) Models</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 Sentient Machines. Built with Docusaurus.</div></div></div></footer><div class="widgetContainer_aqAC"><button class="glowingCircleButton_SIZh">ðŸ’¬</button></div></div>
</body>
</html>