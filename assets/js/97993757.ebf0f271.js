"use strict";(globalThis.webpackChunksentient_machines_handbook=globalThis.webpackChunksentient_machines_handbook||[]).push([[967],{7014:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"vla/voice-command","title":"Voice Command: Integrating OpenAI Whisper","description":"Natural language understanding is a critical component for robots to interact intuitively with humans. The first step in processing voice commands is accurate speech-to-text (STT) transcription. OpenAI Whisper is a robust and highly accurate STT model that excels at transcribing spoken language into text, even in noisy environments or with varied accents. Integrating Whisper into a robotics platform allows humans to issue commands verbally, paving the way for more natural human-robot interaction.","source":"@site/docs/05-vla/02-voice-command.md","sourceDirName":"05-vla","slug":"/vla/voice-command","permalink":"/BOOK-WITH-RAGCHATBOT/docs/vla/voice-command","draft":false,"unlisted":false,"editUrl":"https://github.com/CodeVoyager007/BOOK-WITH-RAGCHATBOT/tree/main/book/docs/05-vla/02-voice-command.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Voice Command: Integrating OpenAI Whisper","sidebar_label":"Voice Command"},"sidebar":"tutorialSidebar","previous":{"title":"Generative Robotics","permalink":"/BOOK-WITH-RAGCHATBOT/docs/vla/generative-robotics"},"next":{"title":"Cognitive Planning","permalink":"/BOOK-WITH-RAGCHATBOT/docs/vla/cognitive-planning"}}');var t=i(4848),a=i(8453);const r={title:"Voice Command: Integrating OpenAI Whisper",sidebar_label:"Voice Command"},o="Voice Command: Integrating OpenAI Whisper",l={},d=[{value:"Why OpenAI Whisper?",id:"why-openai-whisper",level:2},{value:"Integration Architecture in ROS 2",id:"integration-architecture-in-ros-2",level:2},{value:"Setting up OpenAI Whisper",id:"setting-up-openai-whisper",level:2},{value:"Installation (Python)",id:"installation-python",level:3},{value:"Basic Usage (Python)",id:"basic-usage-python",level:3},{value:"ROS 2 Whisper Node (Conceptual)",id:"ros-2-whisper-node-conceptual",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-command-integrating-openai-whisper",children:"Voice Command: Integrating OpenAI Whisper"})}),"\n",(0,t.jsxs)(n.p,{children:["Natural language understanding is a critical component for robots to interact intuitively with humans. The first step in processing voice commands is accurate speech-to-text (STT) transcription. ",(0,t.jsx)(n.strong,{children:"OpenAI Whisper"})," is a robust and highly accurate STT model that excels at transcribing spoken language into text, even in noisy environments or with varied accents. Integrating Whisper into a robotics platform allows humans to issue commands verbally, paving the way for more natural human-robot interaction."]}),"\n",(0,t.jsx)(n.h2,{id:"why-openai-whisper",children:"Why OpenAI Whisper?"}),"\n",(0,t.jsx)(n.p,{children:"Before Whisper, many STT systems struggled with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accuracy"}),": Often made errors with colloquialisms, jargon, or non-standard accents."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Performance dropped significantly in noisy environments."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual Support"}),": Limited to a few languages."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Whisper, trained on a massive dataset of diverse audio and text, overcomes many of these limitations, offering:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High Accuracy"}),": State-of-the-art performance across a wide range of audio conditions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual Transcription"}),": Supports transcription in many languages."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Identification"}),": Can automatically identify the spoken language."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness to Noise"}),": Performs well even with background noise."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-architecture-in-ros-2",children:"Integration Architecture in ROS 2"}),"\n",(0,t.jsx)(n.p,{children:"To integrate Whisper into a ROS 2 system, we typically follow these steps:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Capture"}),": A ROS 2 node (e.g., a custom driver or a generic audio capture node) records audio from a microphone and publishes it as a stream of raw audio data (e.g., ",(0,t.jsx)(n.code,{children:"audio_common_msgs/msg/AudioData"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Whisper Node"}),": A dedicated ROS 2 node subscribes to this audio stream, processes it with the Whisper model, and publishes the transcribed text."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Interpretation"}),": A separate node subscribes to the transcribed text and interprets it as robot commands, potentially using an LLM for natural language understanding (as discussed in the next chapter)."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"setting-up-openai-whisper",children:"Setting up OpenAI Whisper"}),"\n",(0,t.jsxs)(n.p,{children:["Whisper can be run locally on your robot's edge device (if it has sufficient compute, e.g., Jetson Orin with a GPU) or on a remote workstation/server. The Hugging Face ",(0,t.jsx)(n.code,{children:"transformers"})," library provides an easy way to load and run Whisper models."]}),"\n",(0,t.jsx)(n.h3,{id:"installation-python",children:"Installation (Python)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install -q transformers torch torchaudio soundfile\npip install -q optimum # For ONNX Runtime optimizations\n"})}),"\n",(0,t.jsx)(n.h3,{id:"basic-usage-python",children:"Basic Usage (Python)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import pipeline\nimport torchaudio\nimport soundfile as sf\n\n# Load a pre-trained Whisper model\n# Options: "tiny", "base", "small", "medium", "large", "large-v2", "large-v3"\n# For edge devices, "tiny" or "base" might be more suitable.\npipe = pipeline("automatic-speech-recognition", model="openai/whisper-tiny")\n\n# Assume you have an audio file\naudio_file = "my_voice_command.wav" # Replace with actual audio input\n\n# If you have audio data as a NumPy array or PyTorch tensor:\n# input_audio, sampling_rate = torchaudio.load(audio_file)\n# if sampling_rate != 16000: # Whisper expects 16kHz audio\n#    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n#    input_audio = resampler(input_audio)\n# transcription = pipe(input_audio.squeeze().numpy(), sampling_rate=16000)["text"]\n\n\n# For file input:\ntranscription = pipe(audio_file)["text"]\nprint(f"Transcription: {transcription}")\n\n# Example of saving a dummy audio file for testing\n# dummy_audio = torch.randn(1, 16000) # 1 second of random noise at 16kHz\n# sf.write("dummy_audio.wav", dummy_audio.squeeze().numpy(), 16000)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"ros-2-whisper-node-conceptual",children:"ROS 2 Whisper Node (Conceptual)"}),"\n",(0,t.jsx)(n.p,{children:"A ROS 2 node for Whisper would typically:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:["Initialize ",(0,t.jsx)(n.code,{children:"rclpy"})," and a ",(0,t.jsx)(n.code,{children:"Node"})]}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create a Subscriber"}),": Listen to an audio topic (e.g., ",(0,t.jsx)(n.code,{children:"/audio/raw"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement a Callback"}),": When audio data arrives:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Buffer the incoming audio."}),"\n",(0,t.jsx)(n.li,{children:"When a sufficient segment of audio is collected (e.g., 5 seconds, or silence is detected), pass it to the Whisper model."}),"\n",(0,t.jsx)(n.li,{children:"Process the audio with the Whisper pipeline."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create a Publisher"}),": Publish the transcribed text (e.g., ",(0,t.jsx)(n.code,{children:"std_msgs/msg/String"}),") to a topic (e.g., ",(0,t.jsx)(n.code,{children:"/robot/voice_command_text"}),")."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n# from audio_common_msgs.msg import AudioData # Assuming this message type is available\nfrom transformers import pipeline\nimport numpy as np\nimport io\nimport soundfile as sf\n\nclass WhisperASRNode(Node):\n\n    def __init__(self):\n        super().__init__('whisper_asr_node')\n        self.get_logger().info('Initializing Whisper ASR Node...')\n        self.asr_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=0) # device=0 for GPU, -1 for CPU\n        self.audio_subscriber = self.create_subscription(\n            # AudioData, # Replace with actual audio message type\n            String, # Using String for a simple demo for now\n            '/audio/raw',\n            self.audio_callback,\n            10\n        )\n        self.text_publisher = self.create_publisher(String, '/robot/voice_command_text', 10)\n        self.audio_buffer = []\n        self.sample_rate = 16000 # Whisper's expected sample rate\n\n        self.get_logger().info('Whisper ASR Node ready.')\n\n    # This callback would process actual audio data\n    def audio_callback(self, msg):\n        # In a real scenario, msg.data would be raw audio bytes\n        # For this demo, let's assume msg.data is a base64 encoded wav or a path\n        # This is a simplification; actual audio handling is more complex\n        \n        # Example: if msg.data is raw audio bytes (numpy array or similar)\n        # self.audio_buffer.extend(msg.data)\n        # For now, let's simulate with a direct transcription\n        self.get_logger().info(f\"Received dummy audio data: {msg.data[:30]}...\")\n        \n        try:\n            # Simulate processing a received audio buffer or file\n            # In a real system, you'd buffer audio and process segments\n            # For demonstration, let's just transcribe the received string as if it's an audio path\n            transcribed_text = self.asr_pipeline(msg.data)[\"text\"] # Assuming msg.data is a path for now\n            self.get_logger().info(f\"Transcribed: {transcribed_text}\")\n            \n            text_msg = String()\n            text_msg.data = transcribed_text\n            self.text_publisher.publish(text_msg)\n        except Exception as e:\n            self.get_logger().error(f\"Error during ASR: {e}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperASRNode()\n    rclpy.spin(whisper_node)\n    whisper_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.em,{children:["Note: The ",(0,t.jsx)(n.code,{children:"audio_callback"})," is a simplification. Real audio processing would involve buffering audio chunks, handling sample rates, and potentially using VAD (Voice Activity Detection) to segment speech."]})}),"\n",(0,t.jsx)(n.p,{children:"By integrating OpenAI Whisper, robots can gain the ability to understand spoken commands, transforming human-robot interaction from a technical chore into a more natural and intuitive experience."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(6540);const t={},a=s.createContext(t);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);