"use strict";(globalThis.webpackChunksentient_machines_handbook=globalThis.webpackChunksentient_machines_handbook||[]).push([[609],{4589:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"vla/cognitive-planning","title":"Cognitive Planning: LLMs as High-Level Planners","description":"Once a robot can understand spoken or written commands, the next challenge is to translate these high-level intentions into a sequence of executable actions. This is the domain of Cognitive Planning, and Large Language Models (LLMs) are emerging as powerful tools to enable robots to perform complex, multi-step tasks by acting as high-level planners.","source":"@site/docs/05-vla/03-cognitive-planning.md","sourceDirName":"05-vla","slug":"/vla/cognitive-planning","permalink":"/BOOK-WITH-RAGCHATBOT/docs/vla/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/CodeVoyager007/BOOK-WITH-RAGCHATBOT/tree/main/book/docs/05-vla/03-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Cognitive Planning: LLMs as High-Level Planners","sidebar_label":"Cognitive Planning"},"sidebar":"tutorialSidebar","previous":{"title":"Voice Command","permalink":"/BOOK-WITH-RAGCHATBOT/docs/vla/voice-command"},"next":{"title":"VLA Models","permalink":"/BOOK-WITH-RAGCHATBOT/docs/vla/vla-models"}}');var a=i(4848),o=i(8453);const s={title:"Cognitive Planning: LLMs as High-Level Planners",sidebar_label:"Cognitive Planning"},l="Cognitive Planning: LLMs as High-Level Planners",r={},c=[{value:"The Robotic Planning Hierarchy",id:"the-robotic-planning-hierarchy",level:2},{value:"LLMs for Cognitive Planning",id:"llms-for-cognitive-planning",level:2},{value:"The LLM-as-Planner Architecture:",id:"the-llm-as-planner-architecture",level:3},{value:"Prompt Engineering for Planning",id:"prompt-engineering-for-planning",level:2},{value:"Example Prompt (Conceptual)",id:"example-prompt-conceptual",level:3},{value:"Example LLM Output:",id:"example-llm-output",level:3},{value:"Challenges and Future Directions",id:"challenges-and-future-directions",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"cognitive-planning-llms-as-high-level-planners",children:"Cognitive Planning: LLMs as High-Level Planners"})}),"\n",(0,a.jsxs)(n.p,{children:["Once a robot can understand spoken or written commands, the next challenge is to translate these high-level intentions into a sequence of executable actions. This is the domain of ",(0,a.jsx)(n.strong,{children:"Cognitive Planning"}),", and ",(0,a.jsx)(n.strong,{children:"Large Language Models (LLMs)"})," are emerging as powerful tools to enable robots to perform complex, multi-step tasks by acting as high-level planners."]}),"\n",(0,a.jsx)(n.h2,{id:"the-robotic-planning-hierarchy",children:"The Robotic Planning Hierarchy"}),"\n",(0,a.jsx)(n.p,{children:"Robot planning typically operates on multiple levels:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High-Level (Cognitive) Planning"}),': Deals with abstract goals and sequences of tasks (e.g., "Make coffee"). This is where LLMs excel.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Mid-Level (Task) Planning"}),': Breaks down tasks into sequences of fundamental robot skills (e.g., "Go to kitchen", "Grasp mug"). Traditional AI Planning (e.g., PDDL) often operates here.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Low-Level (Motion) Planning"}),": Generates collision-free trajectories for the robot's end-effectors and joints (e.g., inverse kinematics, path smoothing)."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Historically, the gap between high-level human intent and low-level robot execution has been difficult to bridge."}),"\n",(0,a.jsx)(n.h2,{id:"llms-for-cognitive-planning",children:"LLMs for Cognitive Planning"}),"\n",(0,a.jsx)(n.p,{children:"LLMs possess remarkable capabilities for:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Common Sense Reasoning"}),": Understanding the typical steps involved in everyday tasks."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Knowledge Retrieval"}),": Accessing a vast corpus of information about objects, environments, and actions."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Text Generation"}),": Producing coherent and structured plans in natural language or a structured format."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"By leveraging these capabilities, an LLM can take a natural language command and generate a high-level plan that a robot's existing skills can then execute."}),"\n",(0,a.jsx)(n.h3,{id:"the-llm-as-planner-architecture",children:"The LLM-as-Planner Architecture:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"User Prompt"}),': A human provides a high-level goal (e.g., "Clean the table").']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LLM Planning"}),': The LLM, given a description of the robot\'s capabilities (a "tool library" or "skill set"), generates a sequence of API calls or abstract actions.',"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'It might also query its internal knowledge or external databases (e.g., "What objects are typically on a table?").'}),"\n",(0,a.jsx)(n.li,{children:"It can ask clarifying questions to the human if the goal is ambiguous."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Skill Execution"}),": Each generated action is passed to a lower-level robot control system, which executes the corresponding pre-defined skill (e.g., a ROS 2 action server for navigation or manipulation)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feedback Loop"}),': The robot provides feedback (e.g., "Reached table," "Grasped bottle") to the LLM, allowing it to track progress, detect failures, and replan if necessary.']}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prompt-engineering-for-planning",children:"Prompt Engineering for Planning"}),"\n",(0,a.jsxs)(n.p,{children:["The performance of an LLM as a planner heavily depends on ",(0,a.jsx)(n.strong,{children:"prompt engineering"}),". The prompt must clearly define:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"The robot's capabilities (available tools/skills with their arguments)."}),"\n",(0,a.jsx)(n.li,{children:"The environment's context (e.g., objects present, their locations)."}),"\n",(0,a.jsx)(n.li,{children:"The desired output format for the plan (e.g., a list of JSON objects, a Python function call)."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-prompt-conceptual",children:"Example Prompt (Conceptual)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"You are a helpful robot assistant. Your goal is to break down complex human requests into a sequence of executable robot commands.\n\nAvailable commands:\n- navigate_to(location_name: str) -> bool: Navigate to a named location. Returns true on success.\n- grasp_object(object_name: str) -> bool: Grasp a specified object. Returns true on success.\n- place_object(location_name: str) -> bool: Place a held object at a named location. Returns true on success.\n- detect_object(object_type: str) -> List[str]: Detect objects of a given type and return their names.\n\nCurrent environment:\n- Locations: kitchen, living_room, bedroom\n- Objects in kitchen: apple, knife, plate\n- Objects in living_room: remote, book\n\nUser request: \"Please bring me the apple from the kitchen and put it on the table in the living room.\"\n\nGenerate a JSON list of commands to achieve this goal. Each command should be a dictionary with 'action' and 'args'.\n\nPlan:\n"})}),"\n",(0,a.jsx)(n.h3,{id:"example-llm-output",children:"Example LLM Output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'[\n  {"action": "navigate_to", "args": {"location_name": "kitchen"}},\n  {"action": "grasp_object", "args": {"object_name": "apple"}},\n  {"action": "navigate_to", "args": {"location_name": "living_room"}},\n  {"action": "place_object", "args": {"location_name": "table"}}\n]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"challenges-and-future-directions",children:"Challenges and Future Directions"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Grounding"}),': Ensuring the LLM\'s abstract plan can be reliably executed by the robot in the physical world. This is the "symbolic grounding problem."']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety"}),": Preventing the LLM from generating unsafe or impossible actions."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Efficiency"}),": Reducing the latency of LLM inference for real-time robotic interaction."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Long-Horizon Planning"}),": Handling very complex tasks with many steps."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Continual Learning"}),": Allowing the robot to learn new skills and update the LLM's understanding of its capabilities."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Cognitive planning with LLMs is rapidly advancing, moving robots beyond pre-programmed routines towards more adaptive, intelligent, and human-understandable autonomy. It's a critical step towards robots that can truly be helpful partners in our daily lives."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>l});var t=i(6540);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);